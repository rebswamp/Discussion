{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting what we have together \n",
    "\n",
    "The broad steps are now quite simple _in principle:_\n",
    "1. Set up the model/pipeline \n",
    "    - **New skill today: dealing with multiple data types in the pipeline using `ColumnTransformer`** <br> <br>\n",
    "    \n",
    "2. Optimize the model's/pipeline's parameters\n",
    "    - E.g. What is the \"best\" strategy for imputing? How should we scale the variables? How many neighbors is the \"best\"? \n",
    "    - Options: `GridSearchCV`, `RandomizedSearchCV`, and SK has some model specific `-CV` functions (e.g. `LassoCV`)\n",
    "    - Tip: print the pipeline to figure out how to specify the parameters keys for  `GridSearchCV` <br> <br>\n",
    "3. Try new combinations of X variables (which to include), X variable transformations (log, non-linear polynomials), and model types (e.g. regression vs logistic), and optimize each \n",
    "    - If you have 40 variables, there are $2^40>billion$ possible combinations. You can't check all of those!    \n",
    "    - Forward selection: \n",
    "        1. Start with empty model and add the variable that generates largest score increase (CV score, AIC, BIC, adj R2)\n",
    "        2. Continue adding variables until some stopping condition is reached \n",
    "    - Backword selection is the opposite. Start with all variables and remove the least helpful. Continue until some stopping condition is reached. Function: `RFECV`.         \n",
    "        - Alternate backwords approaches: `LassoCV` and `SelectFromModel`\n",
    "    - [`sklearn.feature_selection`](https://scikit-learn.org/stable/modules/feature_selection.html) has a bunch of options and examples to show you different approaches for feature selection. Most can be used in a pipeline! :)\n",
    "    \n",
    "    ```python\n",
    "    reg = Pipeline([\n",
    "                      ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))), # or SFM(LassoCV()) \n",
    "                      ('reg', LinearRegression())\n",
    "                    ])\n",
    "    reg.fit(X, y)\n",
    "    ```\n",
    "    <br> <br>\n",
    "4. Compare all the optimized models\n",
    "\n",
    "    ```python\n",
    "    <build list of models>\n",
    "    for model in models:\n",
    "        cross_validate(model, X, y, cv, ...)\n",
    "    ```\n",
    "\n",
    "5. Save the model as an OBJECT others can load and use quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Those 5 steps in pseudo code\n",
    "\n",
    "```python\n",
    "imports \n",
    "load data\n",
    "\n",
    "########################################################################################\n",
    "# STEP 0: EDA\n",
    "########################################################################################\n",
    "\n",
    "Obviously, explore the data and use best practices throughout. This is just pseudo code,\n",
    "not a fully fleshed out \"fill in the blanks\" template\n",
    "\n",
    "########################################################################################\n",
    "# STEP 1: build a pipeline with data cleaning and an estimator\n",
    "########################################################################################\n",
    "\n",
    "# after this, I quickly run pipe_modelName.fit() and pipe_modelName.predict()  \n",
    "# to make sure this works before going forward, but then delete those commands\n",
    "\n",
    "pipe_modelName = make_pipeline(<a sequence of data steps, and the last step is a model>)  \n",
    "\n",
    "########################################################################################\n",
    "# STEP 2: optimize the pipeline\n",
    "########################################################################################\n",
    "\n",
    "# this is the GridSearchCV approach - manually set up the param&value combos to try\n",
    "# doc + examples: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV \n",
    "\n",
    "param_grid = {'stepname__paramname':[val1,val2,...,valN]} # params to try\n",
    "cv = ...                                                  # what folds to use\n",
    "grid = GridSearchCV(pipe_modelName, param_grid,cv,...)    # set up optimizer\n",
    "grid.fit(X,y)                              # fit grid like a \"normal model obj\"\n",
    "optimal_vrs_of_model1 = grid.best_params_  # grid now has new features. save best model\n",
    "\n",
    "# part of this optimization step is picking the best model features\n",
    "\n",
    "########################################################################################\n",
    "# STEP 3: NOW MOVING BEYOND THAT,YOU SHOULD TRY OTHER THINGS! \n",
    "#           (WHAT ARE THE ODDS YOUR FIRST PASS CAN'T BE BEAT?)\n",
    "########################################################################################\n",
    "\n",
    "# MODEL #2\n",
    "# build a new pipeline (e.g. change the model type, which vars to use, how to modify\n",
    "# the vars)\n",
    "# and repeat the pipeline optimization. save the optimal vrs of that model.\n",
    "\n",
    "# MODEL #3\n",
    "# again...\n",
    "\n",
    "...\n",
    "\n",
    "# MODEL #N:\n",
    "# again...\n",
    "\n",
    "########################################################################################\n",
    "# STEP 4: Compare the optimized models\n",
    "########################################################################################\n",
    "\n",
    "# In practice, I'd actually loop through the models with a for-loop and print\n",
    "# the name/scores nicely, but this is just pseudo code\n",
    "\n",
    "cross_validate(optimal_vrs_of_model1,...)   \n",
    "cross_validate(optimal_vrs_of_model2,...) \n",
    "...\n",
    "cross_validate(optimal_vrs_of_modelN,...) \n",
    "\n",
    "########################################################################################\n",
    "# STEP 5: Finishing up\n",
    "########################################################################################\n",
    "\n",
    "# summarize your preferred model (print stats, visual support backing your choice)\n",
    "# save the model as an OBJECT others can load and use quickly\n",
    "\n",
    "we will do this in a minute!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New skill #1: Dealing with multiple variable types\n",
    "\n",
    "### Simple pipelines fail on real world data\n",
    "\n",
    "A pipeline from the last lecture was\n",
    "```python\n",
    "knn_pipe2 = make_pipeline(\n",
    "                        SimpleImputer(strategy='mean'),\n",
    "                        preprocessing.StandardScaler(),  # clean the data\n",
    "                        KNeighborsClassifier()           # model\n",
    "                        )\n",
    "```\n",
    "\n",
    "**The problem is that this won't work if the data has any string (data type = 'object') variables.** Real data usually has \n",
    "- numeric variables that are continuous,\n",
    "- numeric variables that are categorical,\n",
    "- string variables that are categorical,\n",
    "- string variables to process with textual analysis,\n",
    "- variables to ignore. \n",
    "\n",
    "The solution is to build a pipeline that can process different variables differently. Below, I get you set up using the assignment data. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn import metrics\n",
    "\n",
    "# DL data\n",
    "url = 'https://github.com/LeDataSciFi/lectures-spr2020/blob/master/assignment_data/Fannie_Mae_Plus_Data.gzip?raw=true'\n",
    "fannie_mae = pd.read_csv(url,compression='gzip') \n",
    "\n",
    "# separate out y var\n",
    "y = fannie_mae['Original_Interest_Rate']\n",
    "fannie_mae.drop('Original_Interest_Rate',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up how each data type will get dealt with\n",
    "\n",
    "Let's start with the continuous numeric variables. Here, I just try a few variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Original_UPB', 'Original_Loan_Term','Original_Debt_to_Income_Ratio']\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the categorical features. Again, just a few variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['Property_type', 'Loan_purpose']\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore',sparse=False))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the column-specific transformations with ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is ready to include in a pipeline with an estimator\n",
    "\n",
    "It's as easy as: `make_pipeline(preprocessor, model_of_your_choice())`. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine preprocessor with estimator\n",
    "pipe_reg = make_pipeline(preprocessor,\n",
    "                        LinearRegression())\n",
    "pipe_reg # look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That concludes step 2 in the pseudo, the real-world pipeline is ready!**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "## Optimizing the overly simply model above\n",
    "\n",
    "Three reasons for doing this: \n",
    "- specifying `param_grid` is just a little different because the pipeline has steps with nested steps\n",
    "- one more example of optimizing a model\n",
    "- you'll see how I'll evaluate your final model\n",
    "\n",
    "Optimizing this pipeline is just like the pseudo code above: set up the parameter grid, then the grid to search, then fit and save the optimized model to an object.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "             'columntransformer__num__imputer__strategy': ['mean', 'median','most_frequent']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note how we accessed the column transformer, 2 underscores, then the num transformer inside it, 2 underscores,  then the imputer step, then the strategy parameter. I wouldn't have known to do this without looking at the `pipe_reg` output above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_reg, param_grid, cv=5,scoring='r2')\n",
    "grid_search.fit(fannie_mae, y)\n",
    "# grid_search.best_params_                   # examined this\n",
    "opt_model_reg = grid_search.best_estimator_  # save best model to an actual model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_START ASIDE: you can quickly check the model object's R2 in-sample (all of your data) and on the CV folds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does this do insample?\n",
    "print(\"In sample:          \",metrics.r2_score(y,\n",
    "                                              opt_model_reg.predict(fannie_mae)\n",
    "                                             ).round(3)) \n",
    "\n",
    "# lol this model generates negative R2 in the CV folds\n",
    "print(\"Validation fold avg:\",cross_validate(opt_model_reg,\n",
    "                                            fannie_mae, y,\n",
    "                                            scoring=['neg_mean_squared_error','r2']\n",
    "                                           )\n",
    "                                           ['test_r2'].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lol. You probably will want to include more variables.\n",
    "\n",
    "_END ASIDE_\n",
    "\n",
    "---\n",
    "\n",
    "Now, you can drop in the code from [the assignment instructions](https://github.com/LeDataSciFi/LeDataSciFi.github.io/blob/master/assignments/asgn06_pred.md) to save this model to a file I'll evaluate. Make sure to ONLY save your best model!\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - Exercise / Breakout time! \n",
    "\n",
    "Let's break off into groups and try to select which variables to include in a model.\n",
    "\n",
    "Groups can try to implement any of the feature selection at the top of the code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
